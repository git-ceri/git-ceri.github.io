<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="IDMP-RMP">
  <meta property="og:title" content="IDMP-RMP"/>
  <meta property="og:description" content="Safe Human-Robot Interactions with Continuous and Differentiable Distance Fields"/>
  <meta property="og:url" content="https://AdrianMuell.github.io/IDMP-RMP/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/teaser_shift.png" />
  <meta property="og:image:width" content="1920"/>
  <meta property="og:image:height" content="1080"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>IDMP-RMP</title>
  <!--link rel="icon" type="image/x-icon" href="static/images/favicon.ico"-->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Safe Human-Robot Interactions with Continuous and Differentiable Distance Fields</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=xjJhXegAAAAJ&hl=en" target="_blank">Usama Ali</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=vd0gTJIAAAAJ&hl=en&oi=sra" target="_blank">Adrian Müller</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=1rX23ysAAAAJ&hl=en&oi=ao" target="_blank">Fouad Sukkar</a><sup>2</sup>,</span>
                    <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=2oalmUIAAAAJ&hl=en&oi=sra" target="_blank">Lan Wu</a><sup>2</sup>,</span>
                    <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=auMQ1JQAAAAJ&hl=en&oi=ao" target="_blank">Tobias Kaupp</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=gokuA08AAAAJ&hl=en&oi=ao" target="_blank">Teresa Vidal-Calleja</a><sup>2</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Technical University of Applied Sciences Würzburg-Schweinfurt<br><sup>2</sup>University of Technology Sydney</span>
                  </div>

<!--                   <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2403.09988v1.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>TBD</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
<!--                   <span class="link-block">
                    <a href="https://github.com/Usaali/IDMP-RMP" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
<!--                 <span class="link-block">
                  <a href="https://arxiv.org/abs/2403.09988v1" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div> -->
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="vidcontainer">
    <!-- Paper video. -->
    <div class="hero-body">
      <iframe src="https://www.youtube.com/embed/sPcBpSNLb0I" frameborder="0"  allowfullscreen class="video"></iframe>
    </div>
  </div>
</section>
<!-- End teaser video -->
<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content">
          <p>
            Human-robot collaboration applications require safe and reactive planning. Euclidean distance fields (EDF) are a promising representation of such dynamic scenes due to their ability to reason about free space and the readily available distance to collision costs. A key challenge for the commonly used discrete EDF representations, however, is the need for differentiable distance fields to produce smooth collision costs and efficient updates of dynamic objects. In this paper, we propose to use a Gaussian Process (GP) distance field-based framework that enables both, differentiable distance fields and fast dynamic scene updates. Moreover, we combine this framework with the Riemannian Motion Policies as a local reactive planner to enable safe human-robot interactions. We design a collision avoidance policy that models the repulsive motion using the distance and gradient fields from our GP. We show our reactive planner in an experiment with a UR5e interacting safely and smoothly with a human.
          </p>
          <img src="static/images/teaser_shift.png" alt="System diagram of our proposed framework.">
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3"><p style="text-align:center">Proposed Framework</p></h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <img src="static/images/framework.png" alt="System diagram of our proposed framework.">
          <p>
            The Figure shows proposed system architecture, where <a href="https://uts-ri.github.io/IDMP/">IDMP</a> takes as input the depth sensor's data and pose. These inputs are used to generate the local Frustum Field which determines the implicit semantics of the scene and is then used to fuse the new observation with the global GPDF. Our RMP policy queries distance and gradient information from the fused global GPDF to generate accelerations which are passed to a controller for execution on the robot.
            The key aspect of the IDMP framework is that it uses a Frustrum Field to fuse and identify the dynamic regions locally before passing the information to the Fused Field that contains the global information. 
            The following figures are showing the internal update process of IDMP. The background displays the distance field within the sensor's field of view generated by the frustum GPDF. While the fused GPDF is trained on all points from the internal global map, the frustum GPDF only utilizes the latest observations, capturing changes in the scene. By querying the frustum GPDF with the fused GPDF's training points, we can directly retrieve implicit semantics based on distance metrics.
            Training points in the fused GPDF are classified as <i>static</i> if their queried distance in the frustum GPDF is below a certain threshold, indicating the object has not moved. Training points are classified as <i>dynamic</i> when this distance exceeds the sensor noise threshold, indicating that the object has moved. For the final case we query newly observed sensor points with the fused GPDF. Those points with distances greater than a certain threshold are classified as <i>new</i> and are fused into the global GPDF.
          </p>
          <div style="display: flex;">
            <div class="center">
              <img src="static/images/Frustum_V11.png" alt="Avatar woman">
            </div>
            <div class="center">
              <img src="static/images/Frustum_V12.png" alt="Avatar woman">
            </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <h2 class="title is-3"><p style="text-align:center">Datasets</p></h2>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <p>
          To evaluate our proposed framework, we quantitatively compare the mapping performance for both static and dynamic scenes with other frameworks. For static incrementally built scene we use the Cow and Lady dataset. For a dynamic scene, we have created our own Gazebo simulation dataset.
        </p>
      </div>
    </div>
    <div style="display: flex;">
      <div class="center">
        <div class="link-block">
          <a href="https://projects.asl.ethz.ch/datasets/doku.php?id=iros2017" target="_blank"
          class="external-link button is-normal is-rounded is-dark">
          Cow and Lady
          </a>
        </div>
        <img src="static/images/cow_lady_gradients.png" style="width:80%" alt="Avatar woman">
      </div>
      <div class="center">
        <div class="link-block">
          <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
          class="external-link button is-normal is-rounded is-dark">
          Gazebo Dynamic
          </a>
        </div>
        <img src="static/images/ball_bw.png" style="width:80%" alt="Avatar woman">
      </div>
    </div>
  </div>
</section> -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3"><p style="text-align:center">Quantitative Results</p></h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <p>
            We evaluate our method in a mock human-robot interaction scene where the robot is tasked to cycle between two waypoints. During the execution, a human enters the workspace and places their arm in the way of the robot.
            We compare the behaviour of our framework against an occupancy-based reactive method implemented in ROS package MoveIt. This baseline method builds an <a href="https://github.com/OctoMap/octomap?tab=readme-ov-file">Octomap</a> which is continuously updated with the sensor input. A trajectory is then planned using the <a href="https://arxiv.org/abs/1507.07602">Bi-directional Fast Marching Tree (BFMT*)</a>.
            During execution the trajectory is checked for possible collisions which then triggers replanning.
          </p>
          <img src="static/images/moveit_vs_rmp_V2.png" alt="Avatar woman">
          <!--p>
            The following figure shows the resulting trajectories for both our method and the baseline. Compared to the baseline our method is able to react more naturally to dynamic obstacles whereas the baseline method awkwardly stops as it replans.
          </p>
          <img src="static/images/moveit_rmp.png" alt="Avatar woman"-->
          <p>
            As can be seen in the following Table the trajectories produced by our method result in much smoother trajectories. Notably the mean squared jerk and change in curvature were approximately 2x and 3x lower, respectively, than the baseline.
          </p>
          <img src="static/images/comparison_rmp_moveit.png" alt="Avatar woman">
          <!--div style="display: flex;">
            <div class="center">
              <img src="static/images/policy_metr.png" alt="Avatar woman">
            </div>
            <div class="center">
              <img src="static/images/policy_acc.png" alt="Avatar woman">
            </div>
        </div-->
      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->

<!-- Video carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3"><p style="text-align:center">Videos</p></h2>       
      <div id="results-carousel" class="carousel results-carousel">
        <!-- <div class="item item-video0">
          <video poster="" id="video0" autoplay controls muted loop height="100%">
            <source src="static/videos/RMPs_Workshop.mp4"
            type="video/mp4">
          </video>
        </div> -->
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/ICRA_distance_gradient.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/ICRA_rmp_static.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/ICRA_rmp_dynamic.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video4">
          <video poster="" id="video4" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/ICRA_rmp_moveit.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video5">
          <video poster="" id="video5" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/ICRA_rmp_pick_place.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Default Statcounter code for IDMP-RMP Paper Page
https://usaali.github.io/IDMP-RMP/ -->
<script type="text/javascript">
  var sc_project=13008488; 
  var sc_invisible=1; 
  var sc_security="c1ecd61f"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js"
  async></script>
  <noscript><div class="statcounter"><a title="Web Analytics"
  href="https://statcounter.com/" target="_blank"><img
  class="statcounter"
  src="https://c.statcounter.com/13008488/0/c1ecd61f/1/"
  alt="Web Analytics"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
  <!-- End of Statcounter Code -->

  </body>
  </html>
